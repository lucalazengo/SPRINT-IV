import os
import pickle
import re
import pandas as pd
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from flask import Flask, render_template, request, jsonify

# Importa√ß√µes do Vertex AI
import vertexai
from vertexai.generative_models import GenerativeModel, Part, HarmCategory, HarmBlockThreshold

# --- Configura√ß√µes da Aplica√ß√£o ---
app = Flask(__name__)

# --- Constantes e Configura√ß√µes de Caminhos ---
MODEL_NAME_SEMANTIC = 'all-mpnet-base-v2' # Para a busca sem√¢ntica
MODEL_NAME_LLM = 'gemini-1.5-pro-002' # Para o LLM Gemini (ajuste conforme necess√°rio)

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, 'data', 'processed')
DATASET_FILENAME = 'embeddings.csv'
EMBEDDINGS_FILENAME = 'embeddings.pkl'
DATASET_PATH = os.path.join(DATA_DIR, DATASET_FILENAME)
EMBEDDINGS_PATH = os.path.join(DATA_DIR, EMBEDDINGS_FILENAME)

# --- Configura√ß√µes do Vertex AI ---
# !! IMPORTANTE !!
# Para DESENVOLVIMENTO, voc√™ pode definir o caminho do JSON aqui, mas NUNCA o versione.
# Em PRODU√á√ÉO, configure a vari√°vel de ambiente GOOGLE_APPLICATION_CREDENTIALS no seu servidor
# ou use a autentica√ß√£o impl√≠cita do ambiente Google Cloud.
GOOGLE_CREDENTIALS_PATH = r"c:/Users/mlzengo/Documents/TJGO/SPRINT IV/br-tjgo-cld-02-09b1b22e65b3.json" 
GOOGLE_PROJECT_ID = "br-tjgo-cld-02"  
GOOGLE_LOCATION = "us-central1"

sentence_model = None
dataset = None
embeddings_global = None
faiss_index = None
enriquecedor_llm = None
resources_loaded = False
expected_model_dim = 0
dataset_len = 0

class EnriquecedorLLM:
    def __init__(self, project_id, location):
        try:
            if "GOOGLE_APPLICATION_CREDENTIALS" not in os.environ and os.path.exists(GOOGLE_CREDENTIALS_PATH):
                os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = GOOGLE_CREDENTIALS_PATH
            
            vertexai.init(project=project_id, location=location)
            print(f"‚úÖ Vertex AI inicializado para projeto {project_id} em {location}.")
            
            # Instru√ß√µes de sistema refinadas para o contexto do tribunal
            self.system_instruction = [
                Part.from_text("Voc√™ √© um assistente jur√≠dico altamente especializado, respondendo em portugu√™s do Brasil."),
                Part.from_text("Sua principal fun√ß√£o √© analisar a pergunta de um usu√°rio e um conjunto de jurisprud√™ncias (contexto) relacionadas √† Direito da Sa√∫de no Estado de Goi√°s para fornecer uma resposta concisa, objetiva e informativa."),
                Part.from_text("A resposta deve ser estruturada da seguinte forma:"),
                Part.from_text("1. **S√≠ntese Geral:** Comece com um par√°grafo resumindo os principais achados das jurisprud√™ncias em rela√ß√£o √† pergunta do usu√°rio. Destaque se h√° um entendimento consolidado, diverg√™ncias, ou se os casos s√£o muito espec√≠ficos."),
                Part.from_text("2. **Detalhes das Jurisprud√™ncias Relevantes (se aplic√°vel e solicitado implicitamente ou se o resumo se beneficiar disso):** Ap√≥s a s√≠ntese, se houver jurisprud√™ncias particularmente ilustrativas ou se a pergunta demandar detalhes, apresente at√© 2-3 casos mais relevantes. Para cada caso detalhado, use o seguinte formato:"),
                Part.from_text("   --- Jurisprud√™ncia Detalhada ---"),
                Part.from_text("   - **Diagn√≥stico Principal:** [extra√≠do do contexto]"),
                Part.from_text("   - **Procedimento/Medica√ß√£o Solicitado(a):** [extra√≠do do contexto]"),
                Part.from_text("   - **Decis√£o Judicial e Justificativa Principal:** [resuma a decis√£o e sua justificativa principal, conforme o contexto]"),
                Part.from_text("   - **CID (se dispon√≠vel):** [extra√≠do do contexto]"),
                Part.from_text("   - **Score de Similaridade com a Pergunta:** [fornecido no contexto]"),
                Part.from_text("   - **Refer√™ncia da Nota T√©cnica:** [Se um link URL for fornecido no contexto para este campo, formate-o como um link clic√°vel em Markdown, por exemplo: [Visualizar Nota T√©cnica](URL_REAL_DO_LINK). Se n√£o houver link ou n√£o for aplic√°vel, indique 'N√£o dispon√≠vel' ou 'N√£o se aplica'.]"),
                Part.from_text("   ---------------------------------"),
                Part.from_text("Se as jurisprud√™ncias fornecidas no contexto n√£o forem suficientes para responder √† pergunta do usu√°rio de forma conclusiva, ou se o contexto estiver vazio, declare isso claramente (ex: 'Com base nas informa√ß√µes fornecidas, n√£o foi poss√≠vel encontrar um entendimento claro ou jurisprud√™ncias suficientemente detalhadas sobre este tema espec√≠fico.' ou 'Nenhuma jurisprud√™ncia foi encontrada para esta consulta.')."),
                Part.from_text("Mantenha um tom formal e objetivo. Evite opini√µes pessoais ou informa√ß√µes n√£o contidas no contexto fornecido (pergunta do usu√°rio e jurisprud√™ncias)."),
                Part.from_text("Use o 'Score de Similaridade' para entender a relev√¢ncia de cada jurisprud√™ncia para a pergunta original, mas n√£o necessariamente o mencione na resposta final, a menos que seja para justificar a escolha de detalhar um caso."),
                Part.from_text("O objetivo √© prover uma an√°lise √∫til e r√°pida para profissionais do direito dentro do tribunal.")
            ]
            
            self.model = GenerativeModel(
                MODEL_NAME_LLM,
                system_instruction=self.system_instruction
            )
            print(f"‚úÖ Modelo LLM ({MODEL_NAME_LLM}) inicializado.")
            self.model_ready = True

        except Exception as e:
            print(f"‚ùå Erro ao inicializar EnriquecedorLLM: {e}")
            # import traceback; traceback.print_exc() # Para debug
            self.model_ready = False
            self.model = None # Garante que o modelo n√£o seja usado

        self.safety_settings = {
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
        }
        self.generation_config = {
            "max_output_tokens": 4096, 
            "temperature": 0.6, 
            "top_p": 0.95,
        }
    
    def gerar_resposta_enriquecida(self, contexto_completo):
        if not self.model_ready or not self.model:
            return "Desculpe, o assistente de enriquecimento de respostas n√£o est√° dispon√≠vel no momento."
        
        print("üîÑ Gerando resposta enriquecida com LLM...")
        
        try:
            
           
            response = self.model.generate_content(
                [contexto_completo], 
                generation_config=self.generation_config,
                safety_settings=self.safety_settings,
                stream=False, 
            )
            
            # Tratamento de resposta e poss√≠veis bloqueios
            if response.candidates and response.candidates[0].content.parts:
                generated_text = response.candidates[0].content.parts[0].text
                print("‚úÖ Resposta do LLM recebida.")
                return generated_text
            else:
                reason = "desconhecida"
                if response.candidates and response.candidates[0].finish_reason:
                    reason = response.candidates[0].finish_reason.name
                elif response.prompt_feedback and response.prompt_feedback.block_reason:
                    reason = response.prompt_feedback.block_reason.name
                print(f"‚ö†Ô∏è Resposta da LLM vazia ou bloqueada. Raz√£o: {reason}")
                if reason == "SAFETY":
                    return "A resposta n√£o p√¥de ser gerada devido a restri√ß√µes de seguran√ßa do conte√∫do."
                return "N√£o foi poss√≠vel gerar uma resposta neste momento. Por favor, tente reformular sua pergunta ou tente mais tarde."

        except Exception as e:
            print(f"‚ùå Erro durante a consulta ao LLM: {e}")
            # import traceback; traceback.print_exc()
            return f"Erro ao consultar o modelo de linguagem: {type(e).__name__}"

# --- Fun√ß√£o Auxiliar de Parsing (sem altera√ß√µes) ---
def extract_field_from_text(text_block, field_label):
    if not isinstance(text_block, str) or not isinstance(field_label, str): return "N/A"
    pattern = rf"(?i){re.escape(field_label)}\s*:\s*(.*?)(?=\n\s*[A-Z√Ä-√öa-z√Ä-√ñ√ò-√∂√∏-√ø][\w\s√Ä-√ñ√ò-√∂√∏-√ø()]*\s*:|\Z)"
    match = re.search(pattern, text_block, re.DOTALL)
    if match:
        value = match.group(1).strip()
        if value.startswith(".."): value = value[2:].strip()
        elif value.startswith("."): value = value[1:].strip()
        return value if value else "N/A"
    return "N/A"

# --- 
def load_resources():
    global sentence_model, dataset, embeddings_global, faiss_index, enriquecedor_llm
    global resources_loaded, expected_model_dim, dataset_len

    resources_loaded = False # Reset
    try:
        print("üöÄ Inicializando Enriquecedor LLM...")
        enriquecedor_llm = EnriquecedorLLM(project_id="br-tjgo-cld-02", location="us-central1")
        if not enriquecedor_llm.model_ready:
            print("‚ö†Ô∏è Enriquecedor LLM n√£o foi inicializado corretamente. Funcionalidade de enriquecimento estar√° desabilitada.")
           

        
        print(f"üöÄ Carregando modelo SentenceTransformer ({MODEL_NAME_SEMANTIC})...")
        sentence_model = SentenceTransformer(MODEL_NAME_SEMANTIC)
        expected_model_dim = sentence_model.get_sentence_embedding_dimension()
        print(f"‚úÖ Modelo SentenceTransformer carregado. Dimens√£o: {expected_model_dim}")

        
        print(f"üöÄ Carregando DataFrame de {DATASET_PATH}...")
        if not os.path.exists(DATASET_PATH):
            print(f"‚ùå Erro: Arquivo do dataset n√£o encontrado: {DATASET_PATH}"); return
        dataset = pd.read_csv(DATASET_PATH)
        dataset_len = len(dataset)
        print(f"‚úÖ DataFrame carregado com {dataset_len} registros.")

        # 4. Carregar e processar Embeddings
        print(f"üöÄ Carregando embeddings de {EMBEDDINGS_PATH}...")
        if not os.path.exists(EMBEDDINGS_PATH):
            print(f"‚ùå Erro: Arquivo de embeddings n√£o encontrado: {EMBEDDINGS_PATH}"); return
        with open(EMBEDDINGS_PATH, 'rb') as f:
            embeddings_data_from_pickle = pickle.load(f)
        print("‚úÖ Embeddings carregados do pickle.")
        
        actual_embeddings_source = None
        if isinstance(embeddings_data_from_pickle, tuple):
            print(f"Dados do pickle s√£o uma TUPLA com {len(embeddings_data_from_pickle)} elemento(s).")
            if embeddings_data_from_pickle and isinstance(embeddings_data_from_pickle[0], (list, np.ndarray)):
                actual_embeddings_source = embeddings_data_from_pickle[0]
                print(f"    üéâ Usando o elemento 0 da tupla como fonte de embeddings.")
            else: print("‚ùå Elemento 0 da tupla n√£o √© lista/array ou tupla est√° vazia."); return
        elif isinstance(embeddings_data_from_pickle, (list, np.ndarray)):
            actual_embeddings_source = embeddings_data_from_pickle
        else: print(f"‚ùå Tipo de dados inesperado ({type(embeddings_data_from_pickle)}) no pickle."); return

        final_embeddings_array = None
        if isinstance(actual_embeddings_source, np.ndarray):
            if actual_embeddings_source.ndim == 2 and actual_embeddings_source.shape[0] == dataset_len and actual_embeddings_source.shape[1] == expected_model_dim:
                final_embeddings_array = actual_embeddings_source.astype('float32')
        elif isinstance(actual_embeddings_source, list):
            if len(actual_embeddings_source) == dataset_len:
                temp_valid = [list(e) for e in actual_embeddings_source if isinstance(e, (list, np.ndarray)) and len(e) == expected_model_dim]
                if len(temp_valid) == dataset_len: final_embeddings_array = np.array(temp_valid, dtype='float32')
        
        if final_embeddings_array is None or final_embeddings_array.size == 0 :
            print("‚ùå Falha ao processar embeddings para NumPy array ou dimens√µes incorretas."); return
        embeddings_global = final_embeddings_array
        print(f"‚úÖ Embeddings processados para array NumPy com shape {embeddings_global.shape}")
        
        faiss.normalize_L2(embeddings_global)
        print("‚úÖ Embeddings normalizados.")
        faiss_index = faiss.IndexFlatIP(embeddings_global.shape[1])
        faiss_index.add(embeddings_global)
        print(f"‚úÖ √çndice FAISS criado com {faiss_index.ntotal} vetores.")
        
        resources_loaded = True # Indica que os recursos principais para busca est√£o OK
        if not (enriquecedor_llm and enriquecedor_llm.model_ready):
            print("‚ö†Ô∏è Aviso: O LLM para enriquecimento de respostas n√£o est√° operacional.")
            # A aplica√ß√£o funcionar√°, mas sem o enriquecimento do LLM.

    except Exception as e:
        import traceback
        print(f"‚ùå Erro cr√≠tico geral ao carregar recursos: {e}")
        traceback.print_exc()
        resources_loaded = False



# Dentro da fun√ß√£o buscar_jurisprudencia_semantica em similarity.py

def buscar_jurisprudencia_semantica(query, top_k=5):
    # Altera√ß√£o aqui: troque 'not dataset' por 'dataset is None'
    if sentence_model is None or faiss_index is None or dataset is None:
        print("‚ö†Ô∏è Aviso: Recursos da busca sem√¢ntica (sentence_model, faiss_index ou dataset) n√£o carregados.")
        return [] # Retorna lista vazia

    print(f"üîé Iniciando busca sem√¢ntica por: '{query}' com top_k={top_k}")
    query_embedding = sentence_model.encode([query], normalize_embeddings=True)
    query_embedding_np = np.array(query_embedding).astype('float32').reshape(1, -1)

    distances, indices = faiss_index.search(query_embedding_np, top_k)
    resultados_extraidos = []
    
    
    field_labels_for_parsing = {
        'diagn√≥stico': 'Diagn√≥stico', 'conclus√£o': 'Conclus√£o',
        'justificativa': 'Justificativa', 
        'cid': 'CID', 'princ√≠pio ativo': 'Princ√≠pio Ativo', 
        'nome comercial': 'Nome Comercial', 'descri√ß√£o': 'Descri√ß√£o', 
        'tipo da tecnologia': 'Tipo da Tecnologia', '√≥rg√£o': '√ìrg√£o', 
        'serventia': 'Serventia'
    }

    for i in range(indices.shape[1]): 
        idx = indices[0, i]
        score = distances[0, i]
        if idx < 0 or idx >= len(dataset): continue
        try:
            row = dataset.iloc[idx]
            texto_completo = str(row['texto']) if pd.notna(row['texto']) else ""
            item = {'texto_original': texto_completo} 
            for key, label_in_text in field_labels_for_parsing.items():
                item[key] = extract_field_from_text(texto_completo, label_in_text)
            
            item['referencia'] = str(row['referencia']) if pd.notna(row['referencia']) else ''
            item['similaridade_busca'] = float(score) if pd.notna(score) else 0.0
            resultados_extraidos.append(item)
        except Exception as e:
            print(f"‚ùå Erro ao processar item da busca sem√¢ntica no √≠ndice {idx}: {e}")
    
    print(f"‚úÖ Busca sem√¢ntica encontrou {len(resultados_extraidos)} resultados.")
    return resultados_extraidos


# --- Rotas Flask ---
@app.route('/')
def home():
    return render_template('index.html')

@app.route('/get_response', methods=['POST'])
def get_chat_response():
    if not resources_loaded: # Checagem geral de recursos
        return jsonify({'error': 'Servi√ßo temporariamente indispon√≠vel (recursos n√£o carregados).'}), 503

    user_query = request.form.get('query')
    if not user_query:
        return jsonify({'error': 'Nenhuma pergunta fornecida.'}), 400

    try:
        # 1. Obter Resultados da Busca Sem√¢ntica Primeiro
        resultados_semanticos = buscar_jurisprudencia_semantica(user_query, top_k=5)

        # 2. Construir o Contexto para o LLM
        contexto_para_llm = f"Pergunta do Usu√°rio: \"{user_query}\"\n\n"
        if not resultados_semanticos:
            contexto_para_llm += "Contexto das Jurisprud√™ncias Encontradas: Nenhuma jurisprud√™ncia espec√≠fica foi encontrada pela busca inicial para esta pergunta."
        else:
            contexto_para_llm += "Contexto das Jurisprud√™ncias Encontradas (analise e use para formular sua resposta):\n"
            for i, res in enumerate(resultados_semanticos):
                contexto_para_llm += f"\n--- Jurisprud√™ncia {i+1} (Score de Similaridade com a pergunta: {res.get('similaridade_busca', 0):.3f}) ---\n"
                # Usar os r√≥tulos que voc√™ listou, pegando os valores do dicion√°rio 'res'
                contexto_para_llm += f"Diagn√≥stico: {res.get('diagn√≥stico', 'N√£o informado')}\n"
                contexto_para_llm += f"Conclus√£o: {res.get('conclus√£o', 'N√£o informado')}\n"
                contexto_para_llm += f"Justificativa: {res.get('justificativa', 'N√£o informado')}\n" 
                contexto_para_llm += f"CID: {res.get('cid', 'N√£o informado')}\n"
                contexto_para_llm += f"Princ√≠pio Ativo: {res.get('princ√≠pio ativo', 'N√£o informado')}\n"
                contexto_para_llm += f"Nome Comercial: {res.get('nome comercial', 'N√£o informado')}\n"
                contexto_para_llm += f"Descri√ß√£o: {res.get('descri√ß√£o', 'N√£o informado')}\n"
                contexto_para_llm += f"Tipo da Tecnologia: {res.get('tipo da tecnologia', 'N√£o informado')}\n"
                contexto_para_llm += f"√ìrg√£o: {res.get('√≥rg√£o', 'N√£o informado')}\n"
                contexto_para_llm += f"Serventia: {res.get('serventia', 'N√£o informado')}\n"
                if res.get('referencia'):
                    contexto_para_llm += f"Refer√™ncia da Nota T√©cnica: {res.get('referencia')}\n"
                # Opcional: incluir trecho do texto original se for curto e relevante
                # texto_orig_curto = res.get('texto_original', '')[:300] # Primeiros 300 chars
                # contexto_para_llm += f"Trecho do Texto Original: {texto_orig_curto}...\n"

        print(f"\n--- CONTEXTO COMPLETO PARA LLM ---\n{contexto_para_llm}\n--------------------------------\n")

        # 3. Chamar o LLM para gerar uma resposta enriquecida
        if enriquecedor_llm and enriquecedor_llm.model_ready:
            resposta_final = enriquecedor_llm.gerar_resposta_enriquecida(contexto_para_llm)
        else:
            resposta_final = "O assistente de enriquecimento de respostas n√£o est√° dispon√≠vel. Seguem os resultados da busca simples:\n\n"
            if resultados_semanticos:
                for i, res in enumerate(resultados_semanticos):
                    resposta_final += f"Resultado {i+1}:\n"
                    for key, value in res.items():
                        if key != 'texto_original' and value != "N/A" and value: # N√£o mostrar texto original completo aqui
                             resposta_final += f"  {key.replace('_', ' ').capitalize()}: {value}\n"
                    resposta_final += "\n"
            else:
                resposta_final = "Nenhuma jurisprud√™ncia encontrada para esta consulta."
        
        # A resposta_final √© uma string (do LLM ou do fallback)
        return jsonify({'response': resposta_final, 'type': 'llm_response'}) # Adiciona type para o frontend

    except Exception as e:
        print(f"‚ùå Erro geral na rota /get_response: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'Ocorreu um erro cr√≠tico ao processar sua solicita√ß√£o: {str(e)}'}), 500

# --- Execu√ß√£o Principal ---
if __name__ == '__main__':
    load_resources() 
    if not resources_loaded:
        print("‚ÄºÔ∏è ATEN√á√ÉO: Alguns recursos essenciais podem n√£o ter sido carregados. A aplica√ß√£o pode n√£o funcionar como esperado.")
    
    app.run(debug=True, host='0.0.0.0', port=5000)